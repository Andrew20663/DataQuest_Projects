{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Guided Project: Predicting House Sale Prices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Linear Regression for Machine Learning course, I started building an intuition for model based learning and saw how linear regression works. I also saw the two different approaches to model fitting (Gradient Descent and Ordinary Least Squares) and some cleaning, transforming, and selecting of certain features to help improve a Linear Regression Model. I will work with the housing data for the city of Ames, Iowa from the years 2006-2010. The pipeline of functions that will allow us to iterate on different models are: \n",
    "\n",
    "   1) train\n",
    "   \n",
    "   2) transform_features()\n",
    "   \n",
    "   3) select_features()\n",
    "   \n",
    "   4) train_and_test()\n",
    "   \n",
    "   5) See both MSE and Average MSE values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57088.25161263909\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('AmesHousing.tsv', delimiter = '\\t')\n",
    "\n",
    "def transform_features(data):\n",
    "    return data\n",
    "\n",
    "def select_features(data): \n",
    "    return data[['Gr Liv Area', 'SalePrice']]\n",
    "\n",
    "def train_test(data):\n",
    "    train = data[:1460]\n",
    "    test = data[1460:]\n",
    "    numeric_train = train.select_dtypes(include = ['integer', 'float'])\n",
    "    numeric_test = test.select_dtypes(include=['integer', 'float'])\n",
    "    attributes = numeric_train.columns.drop('SalePrice')\n",
    "    \n",
    "    lr = linear_model.LinearRegression()\n",
    "    lr.fit(train[attributes], train['SalePrice'])\n",
    "    prediction = lr.predict(test[attributes])\n",
    "    mse = mean_squared_error(prediction, test['SalePrice'])\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "    \n",
    "transformed_feats = transform_features(data)\n",
    "selected_feats = select_features(transformed_feats)\n",
    "rmse = train_test(selected_feats)\n",
    "print(rmse)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating those functions, I can now start removing features with missing values, analyze some categorical features, and transform both text and numericla columns. I will also update the transform_features function so that for any column from the data frame with more than 25% of missing values is not included in the analysis. The goal of the transform_features function is to mainly: \n",
    "\n",
    "   1) Remove features that we don't want to use in the model, based on \n",
    "      the numnber of missing values or data leakage. \n",
    "   \n",
    "   2) Transform features into the proper format. \n",
    "   \n",
    "   3) Create new features by incorporating other features. \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    64\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove missing values\n",
    "missing_values = data.isnull().sum()\n",
    "drop_missing_column = missing_values[(missing_values > len(data)/20)].sort_values()\n",
    "data = data.drop(drop_missing_column.index, axis = 1)\n",
    "\n",
    "#Remove missing text\n",
    "text_missing = data.select_dtypes(include = ['object']).isnull().sum().sort_values(ascending = False)\n",
    "drop_column_2 = text_missing[text_missing > 0]\n",
    "data = data.drop(drop_column_2.index, axis = 1)\n",
    "\n",
    "#Find values to impute to missing values\n",
    "numeric_missing = data.select_dtypes(include = ['float', 'int']).isnull().sum()\n",
    "numeric_impute = numeric_missing[(numeric_missing < len(data)/20) & (numeric_missing >0)].sort_values()\n",
    "numeric_impute\n",
    "\n",
    "#Fill in missing values\n",
    "imputing_value = data[numeric_impute.index].mode().to_dict(orient = 'records')[0]\n",
    "imputing_value\n",
    "\n",
    "data = data.fillna(imputing_value)\n",
    "data.isnull().sum().value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55275.36731241307\n"
     ]
    }
   ],
   "source": [
    "#Update all functions specifically the transform_features() function\n",
    "\n",
    "def transform_features(data):\n",
    "    #Remove missing values\n",
    "    missing_values = data.isnull().sum()\n",
    "    drop_missing_column = missing_values[(missing_values > len(data)/20)].sort_values()\n",
    "    data = data.drop(drop_missing_column.index, axis = 1)\n",
    "\n",
    "    #Remove missing text\n",
    "    text_missing = data.select_dtypes(include = ['object']).isnull().sum().sort_values(ascending = False)\n",
    "    drop_column_2 = text_missing[text_missing > 0]\n",
    "    data = data.drop(drop_column_2.index, axis = 1)\n",
    "\n",
    "    #Find values to impute to missing values\n",
    "    numeric_missing = data.select_dtypes(include = ['float', 'int']).isnull().sum()\n",
    "    numeric_impute = numeric_missing[(numeric_missing < len(data)/20) & (numeric_missing >0)].sort_values()\n",
    "    numeric_impute\n",
    "\n",
    "    #Fill in missing values\n",
    "    imputing_value = data[numeric_impute.index].mode().to_dict(orient = 'records')[0]\n",
    "    imputing_value\n",
    "\n",
    "    data = data.fillna(imputing_value)\n",
    "    data.isnull().sum().value_counts()\n",
    "\n",
    "    #Start feature engineering\n",
    "    year_sold = data['Yr Sold'] - data['Year Built']\n",
    "    year_sold[year_sold < 0 ]\n",
    "\n",
    "    year_remodeled = data['Yr Sold'] - data['Year Remod/Add']\n",
    "    year_remodeled[year_remodeled < 0]\n",
    "\n",
    "    data['Years Before Sale'] = year_sold\n",
    "    data['Years Since Remod'] = year_remodeled\n",
    "\n",
    "    #Drop these row numbers\n",
    "    data = data.drop([1702, 2180, 2181 ], axis = 0)\n",
    "\n",
    "    #Drop these columns \n",
    "    data = data.drop(['PID', 'Order','Mo Sold', 'Sale Condition', 'Sale Type', 'Year Built', 'Year Remod/Add'], axis = 1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def select_features(data): \n",
    "    return data[['Gr Liv Area', 'SalePrice']]\n",
    "\n",
    "def train_test(data):\n",
    "    train = data[:1460]\n",
    "    test = data[1460:]\n",
    "    numeric_train = train.select_dtypes(include = ['integer', 'float'])\n",
    "    numeric_test = test.select_dtypes(include=['integer', 'float'])\n",
    "    attributes = numeric_train.columns.drop('SalePrice')\n",
    "    \n",
    "    lr = linear_model.LinearRegression()\n",
    "    lr.fit(train[attributes], train['SalePrice'])\n",
    "    prediction = lr.predict(test[attributes])\n",
    "    mse = mean_squared_error(prediction, test['SalePrice'])\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "    \n",
    "transformed_feats = transform_features(data)\n",
    "selected_feats = select_features(transformed_feats)\n",
    "rmse = train_test(selected_feats)\n",
    "print(rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate the correlations of the features to the SalePrice column\n",
    "numerical_dataframe = transformed_feats.select_dtypes(include=['int', 'float'])\n",
    "numerical_dataframe.head()\n",
    "correlation = numerical_dataframe.corr()['SalePrice'].abs().sort_values()\n",
    "correlation\n",
    "\n",
    "#Filter out all correlations > 0.4\n",
    "correlation[correlation > 0.4]\n",
    "\n",
    "#Drop Columns with correlation < 0.4\n",
    "transformed_feats = transformed_feats.drop(correlation[correlation < 0.4].index, axis = 1)\n",
    "\n",
    "#Categorical Columns \n",
    "categorical_features = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\", \"Lot Config\", \"Neighborhood\", \n",
    "                    \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \n",
    "                    \"Exterior 2nd\", \"Mas Vnr Type\", \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\", \n",
    "                    \"Misc Feature\", \"Sale Type\", \"Sale Condition\"]\n",
    "\n",
    "#See which categorical columns I keep\n",
    "\n",
    "updated_categorical_features = []\n",
    "\n",
    "for col in categorical_features:\n",
    "    if col in transformed_feats.columns: \n",
    "        updated_categorical_features.append(col)\n",
    "        \n",
    "unique_counts = transformed_feats[updated_categorical_features].apply(lambda col: len(col.value_counts())).sort_values()\n",
    "non_unique_columns = unique_counts[unique_counts > 10].index\n",
    "transformed_data = transformed_feats.drop(non_unique_columns, axis = 1)\n",
    "\n",
    "text_columns = transformed_data.select_dtypes(include = ['object'])\n",
    "for col in text_columns: \n",
    "    transformed_data[col] = transformed_data[col].astype('category')\n",
    "    \n",
    "transformed_data = pd.concat([transformed_data, pd.get_dummies(transformed_data.select_dtypes(include = ['category']))], axis = 1).drop(text_columns, axis = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40244.20587914921, 31895.125100315996, 29222.710164768854, 30184.847827492475]\n",
      "32886.72224293163\n"
     ]
    }
   ],
   "source": [
    "#Update all functions specifically the transform_features() function\n",
    "\n",
    "def transform_features(data):\n",
    "    #Remove missing values\n",
    "    missing_values = data.isnull().sum()\n",
    "    drop_missing_column = missing_values[(missing_values > len(data)/20)].sort_values()\n",
    "    data = data.drop(drop_missing_column.index, axis = 1)\n",
    "\n",
    "    #Remove missing text\n",
    "    text_missing = data.select_dtypes(include = ['object']).isnull().sum().sort_values(ascending = False)\n",
    "    drop_column_2 = text_missing[text_missing > 0]\n",
    "    data = data.drop(drop_column_2.index, axis = 1)\n",
    "\n",
    "    #Find values to impute to missing values\n",
    "    numeric_missing = data.select_dtypes(include = ['float', 'int']).isnull().sum()\n",
    "    numeric_impute = numeric_missing[(numeric_missing < len(data)/20) & (numeric_missing >0)].sort_values()\n",
    "    numeric_impute\n",
    "\n",
    "    #Fill in missing values\n",
    "    imputing_value = data[numeric_impute.index].mode().to_dict(orient = 'records')[0]\n",
    "    imputing_value\n",
    "\n",
    "    data = data.fillna(imputing_value)\n",
    "    data.isnull().sum().value_counts()\n",
    "\n",
    "    #Start feature engineering\n",
    "    year_sold = data['Yr Sold'] - data['Year Built']\n",
    "    year_sold[year_sold < 0 ]\n",
    "\n",
    "    year_remodeled = data['Yr Sold'] - data['Year Remod/Add']\n",
    "    year_remodeled[year_remodeled < 0]\n",
    "\n",
    "    data['Years Before Sale'] = year_sold\n",
    "    data['Years Since Remod'] = year_remodeled\n",
    "\n",
    "    #Drop these row numbers\n",
    "    data = data.drop([1702, 2180, 2181 ], axis = 0)\n",
    "\n",
    "    #Drop these columns \n",
    "    data = data.drop(['PID', 'Order','Mo Sold', 'Sale Condition', 'Sale Type', 'Year Built', 'Year Remod/Add'], axis = 1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def select_features(data, coeff_threshold = 0.4, unique_threshold = 10): \n",
    "    #Calculate the correlations of the features to the SalePrice column\n",
    "    numerical_dataframe = data.select_dtypes(include=['int', 'float'])\n",
    "    numerical_dataframe.head()\n",
    "    correlation = numerical_dataframe.corr()['SalePrice'].abs().sort_values()\n",
    "    data = data.drop(correlation[correlation < coeff_threshold].index, axis = 1)\n",
    "\n",
    "    #Categorical Columns \n",
    "    categorical_features = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\", \"Lot Config\", \"Neighborhood\", \n",
    "                    \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \n",
    "                    \"Exterior 2nd\", \"Mas Vnr Type\", \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\", \n",
    "                    \"Misc Feature\", \"Sale Type\", \"Sale Condition\"]\n",
    "\n",
    "    #See which categorical columns I keep\n",
    "\n",
    "    updated_categorical_features = []\n",
    "\n",
    "    for col in categorical_features:\n",
    "        if col in transformed_feats.columns: \n",
    "            updated_categorical_features.append(col)\n",
    "        \n",
    "    unique_counts = data[updated_categorical_features].apply(lambda col: len(col.value_counts())).sort_values()\n",
    "    non_unique_columns = unique_counts[unique_counts > 10].index\n",
    "    data = data.drop(non_unique_columns, axis = 1)\n",
    "\n",
    "    text_columns = data.select_dtypes(include = ['object'])\n",
    "    for col in text_columns: \n",
    "        data[col] = data[col].astype('category')\n",
    "    \n",
    "    data = pd.concat([data, pd.get_dummies(data.select_dtypes(include = ['category']))], axis = 1).drop(text_columns, axis = 1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def train_and_test(data, k = 0):\n",
    "    numeric_data = data.select_dtypes(include = ['int', 'float'])\n",
    "    features = numeric_data.columns.drop('SalePrice')\n",
    "    lr = linear_model.LinearRegression()\n",
    "    \n",
    "    if k==0:\n",
    "        train = data[:1460]\n",
    "        test = data[1460:]\n",
    "        lr.fit(train[attributes], train['SalePrice'])\n",
    "        prediction = lr.predict(test[attributes])\n",
    "        mse = mean_squared_error(prediction, test['SalePrice'])\n",
    "        rmse = np.sqrt(mse)\n",
    "        return rmse\n",
    " \n",
    "    if k==1: \n",
    "        shuffled_data = data.sample(frac = 1, )\n",
    "        train = data[:1460]\n",
    "        test =  data[1460:]\n",
    "        lr.fit(train[attributes], train['SalePrice'])\n",
    "        prediction = lr.predict(test[attributes])\n",
    "        mse = mean_squared_error(prediction, test['SalePrice'])\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        lr.fit(test[features], test['SalePrice'])\n",
    "        prediction2 = lr.predict(train[features])\n",
    "        mse2 = mean_squared_error(prediction2, train['SalePrice'])\n",
    "        rmse2 = np.sqrt(mse2)\n",
    "        avg_rmse = np.mean(rmse, rmse2)\n",
    "        return avg_rmse\n",
    "    \n",
    "    else: \n",
    "        kf = KFold(n_splits = k, shuffle = True)\n",
    "        rmse_values = []\n",
    "        for train_index, test_index in kf.split(data):\n",
    "            train = data.iloc[train_index]\n",
    "            test = data.iloc[test_index]\n",
    "            lr.fit(train[features], train['SalePrice'])\n",
    "            predictions = lr.predict(test[features])\n",
    "            mse = mean_squared_error(predictions, test['SalePrice'])\n",
    "            rmse = np.sqrt(mse)\n",
    "            rmse_values.append(rmse)\n",
    "        print(rmse_values)\n",
    "        avg_rmse = np.mean(rmse_values)\n",
    "        return avg_rmse\n",
    "    \n",
    "data = transform_features(data)\n",
    "data = select_features(data)\n",
    "rmse = train_and_test(data, k = 4)\n",
    "print(rmse)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
